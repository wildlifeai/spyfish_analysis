{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186df9e3",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Extract-BUV-survey-site-info-from-Deployment/Video-analysis-sheet\" data-toc-modified-id=\"Extract-BUV-survey-site-info-from-Deployment/Video-analysis-sheet-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Extract BUV survey site info from Deployment/Video analysis sheet</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-file/sheet-from-to-extract-sites\" data-toc-modified-id=\"Import-file/sheet-from-to-extract-sites-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import file/sheet from to extract sites</a></span></li><li><span><a href=\"#Read-file-into-dataframe,-keep-relevant-columns\" data-toc-modified-id=\"Read-file-into-dataframe,-keep-relevant-columns-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Read file into dataframe, keep relevant columns</a></span></li><li><span><a href=\"#extract-SiteID\" data-toc-modified-id=\"extract-SiteID-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>extract SiteID</a></span></li><li><span><a href=\"#Extract-lat-&amp;-long\" data-toc-modified-id=\"Extract-lat-&amp;-long-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Extract lat &amp; long</a></span></li><li><span><a href=\"#Fix-SiteNames,-observed-typos,-and-relevant-capitalisation\" data-toc-modified-id=\"Fix-SiteNames,-observed-typos,-and-relevant-capitalisation-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Fix SiteNames, observed typos, and relevant capitalisation</a></span></li><li><span><a href=\"#Keep-only-the-relevant-columns\" data-toc-modified-id=\"Keep-only-the-relevant-columns-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Keep only the relevant columns</a></span></li><li><span><a href=\"#Export-site-info-into-csv\" data-toc-modified-id=\"Export-site-info-into-csv-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Export site info into csv</a></span></li><li><span><a href=\"#Check-which-SiteNames-are-missing\" data-toc-modified-id=\"Check-which-SiteNames-are-missing-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Check which SiteNames are missing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last changed 2024.12.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed089e3",
   "metadata": {},
   "source": [
    "# Extract BUV survey site info from Deployment/Video analysis sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44bf18",
   "metadata": {},
   "source": [
    "This notebooks is part of the 2024 Spyfish data cleaning and is used to migrate information about the sites of the BUV drops from Excel spreadsheets to a consolidated \"BUV Survey Sites\" sharepoint list. The site-related information from the sheets is cleaned up before we upload the information to the sharepoint list.\n",
    "\n",
    "This notebook currently works for extracting Site info from files with this kind of format [BUV MOKES 04](https://docnz.sharepoint.com/:x:/r/teams/SpyfishAotearoa/_layouts/15/Doc.aspx?sourcedoc=%7B3488D733-A46B-4DF4-8394-C111F1766355%7D&file=BUV%20Mokes%2004%20-%20DOCDM-54594.xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c217ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa58585",
   "metadata": {},
   "source": [
    "## Import file/sheet from to extract sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_chooser = FileChooser(title='<b>Select the file from which to extract the sites</b>')\n",
    "display(file_chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_file_name = file_chooser.selected\n",
    "assert site_file_name != None, \"Select site_file_name in the cell above.\"\n",
    "print(f\"The site_file_name is {site_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = pd.ExcelFile(site_file_name).sheet_names\n",
    "for i, e in enumerate(tabs):\n",
    "    print(i, e)\n",
    "\n",
    "FILE_NUM = int(input(\"select sheet you want to process: \"))\n",
    "sheet_name_select = tabs[FILE_NUM]\n",
    "print(\"\\nselected sheet name: \", sheet_name_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411a471",
   "metadata": {},
   "source": [
    "## Read file into dataframe, keep relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df= pd.read_excel(site_file_name, sheet_name=sheet_name_select )\n",
    "print(site_df.columns)\n",
    "site_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATION is the column containing the Site Names\n",
    "print(\"The count and list of the existing site IDs\")\n",
    "len(site_df[\"STATION\"].unique()), site_df[\"STATION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96621f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert column names that contain relevant Site information\n",
    "site_df = site_df[['STATION', 'SITE', 'DATE', 'TIME', 'DEPTH', 'LAT', 'LONG', 'AREA', 'HABITAT', 'Stand']]\n",
    "\n",
    "# As the count columns get dropped, we can delete the empty rows, meaning that each station now features only once\n",
    "# check that dropping all empty rows worked, they are now\n",
    "print(len(site_df))\n",
    "site_df = site_df.dropna()\n",
    "print(len(site_df))\n",
    "print(f\"The number of sites is: {len(site_df)}\")\n",
    "site_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc280475",
   "metadata": {},
   "source": [
    "## extract SiteID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_RESERVE_CODE = \"ABC\"\n",
    "\n",
    "def get_SiteID(site_name):\n",
    "    return f\"{CURRENT_RESERVE_CODE}_{site_name.split('-')[-1].zfill(3)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7224f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df[\"SiteID\"] = site_df[\"STATION\"].apply(get_SiteID)\n",
    "site_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e0fa1",
   "metadata": {},
   "source": [
    "## Extract lat & long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611163a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lat_long(val):\n",
    "    deg, minutes, seconds = val.split(\" \")\n",
    "    new_val = int(deg) + int(minutes) / 60 + int(seconds) / 3600\n",
    "    # for NZ locations\n",
    "    if new_val < 100:\n",
    "        new_val *= -1\n",
    "    return new_val\n",
    "\n",
    "# test cases\n",
    "# print(convert_lat_long(\"35 56 66\"))\n",
    "# print(convert_lat_long(\"175 08 78\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df[\"Latitude\"] = site_df[\"LAT\"].apply(convert_lat_long).round(6)\n",
    "site_df[\"Longitude\"] = site_df[\"LONG\"].apply(convert_lat_long).round(6)\n",
    "site_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109195ab",
   "metadata": {},
   "source": [
    "## Fix SiteNames, observed typos, and relevant capitalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_names(name):\n",
    "    name_list = name.split(\" \")\n",
    "    \n",
    "    # TODO: make it more robust by either adding more words or better instead keeping words \n",
    "    # such as \"of\", \"and\", etc lowercase, and capitalizing everything else\n",
    "    for i,w in enumerate(name_list):\n",
    "        if w in [\"bay\", \"slot\", \"twins\", \"point\", \"site\", \"rock\", \"cave\", \"greenstone\"]:\n",
    "            name_list[i] = w.capitalize()\n",
    "        \n",
    "        # fix specific typos\n",
    "        if w == \"Roxk\":\n",
    "            name_list[i] = \"Rock\"\n",
    "\n",
    "    return \" \".join(name_list)\n",
    "\n",
    "site_df[\"SiteName\"] = site_df[\"SITE\"].apply(fix_names)\n",
    "site_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da9658",
   "metadata": {},
   "source": [
    "## Keep only the relevant columns\n",
    "Review the dataframe before exporting into a file in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed can add more columns\n",
    "site_df_to_extract = site_df[['SiteID', 'SiteName', 'Latitude', 'Longitude']]\n",
    "site_df_to_extract.reset_index(drop=True)\n",
    "site_df_to_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ef34c",
   "metadata": {},
   "source": [
    "## Export site info into csv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_annotations(df_with_vals, file_name, export_csv_file_name=None):\n",
    "    if not export_csv_file_name:\n",
    "        export_file_name = os.path.basename(file_name)\n",
    "        export_file_name = export_file_name[:export_file_name.find('.')]\n",
    "        export_csv_file_name = f\"sites_{export_file_name}.csv\"\n",
    " \n",
    "   # make export folder in current folder \n",
    "    path_to_export = os.path.join(os.path.dirname(file_name), \"export\")\n",
    "    print(path_to_export)\n",
    "    os.makedirs(path_to_export, exist_ok=True)\n",
    "    export_location = os.path.join(path_to_export, export_csv_file_name)\n",
    "    \n",
    "    print(f\"Exporting data to file: '{export_location}'\")\n",
    "    df_with_vals.to_csv(export_location) \n",
    "    \n",
    "\n",
    "print(f\"Showing sample of export with shape: {site_df_to_extract.shape}\")\n",
    "display(site_df_to_extract.sample(10))\n",
    "export_to_annotations(site_df_to_extract, site_file_name)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858f965",
   "metadata": {},
   "source": [
    "## Check which SiteNames are missing\n",
    "\n",
    "Copy paste the names from the BUV SiteNames on Sharepoint to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the sites that are currently on the Sharepoint or any other list, checking for completeness\n",
    "existing_sites = \"\"\"List of Exisitng Sites\n",
    "Site 2\n",
    "Site 3\n",
    "Site 4\"\"\".split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38832aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names = list(site_df_to_extract[\"SiteName\"])\n",
    "site_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979cd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_existing_sites = set(existing_sites)\n",
    "set_site_names = set(site_names)\n",
    "in_existing = sorted(list(set_existing_sites - set_site_names))\n",
    "in_mok_file = sorted(list(set_site_names - set_existing_sites))\n",
    "\n",
    "print(\"Present in BUV Sites sharepoint\\n\", in_existing)\n",
    "print(\"Present in processed file\\n\", in_mok_file)\n",
    "print(len(in_existing))\n",
    "print(len(in_mok_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f50a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
